---
title: "AI-Powered Economic Research workflows"
format:
  revealjs:
    theme: default
    logo: pics/aivaluation_per_employee.png
    footer: "AI Agents & Secure Virtual Labs"
    incremental: false
    embed-resources: true
    self-contained: true
  beamer:
    theme: default
    fontsize: 8pt
    aspectratio: 169
    header-includes: |
      \geometry{margin=0.5in}
editor: visual
---



## Augmented Research Workflows {.smaller}

ğŸ¯ **Goal:** To introduce the concept of AI as a collaboration tool, and AI agents as a powerful new tool for improving research workflows. Want to get people thinking about how to use AI to improve their research workflows 
while highlighting limitations and potential. 





## ğŸ“ˆ The Evolving Landscape {.smaller}

1. ğŸ“š **Traditional Workflow**: Manual literature reviews, Researcher driven coding 

2. ğŸš€ **New Wave: LLM-powered AI Augmentation**: AI as collaborative partner, mostly through a chat interface. Similar to early days of "googling". What are peoples mental models of how AI works?

3. ğŸ¯ **Next wave: LLM powered by tools, e.g. AI agents**: AI Agents as workflow enablers


## ğŸ¤– Demystifying AI Agents {.smaller}

ğŸ” What are these new tools? 

AI Agents are mostly autonomous systems built on LLMs by giving them access to tools that can:

- ğŸ¯ Manage context, e.g. using memory to keep track of previous steps

- ğŸ“ Create detailed plans

- ğŸ› ï¸ Use various tools (e.g. web search, code execution, data analysis, write reports)

- ğŸ”„ Execute multi-step tasks

## ğŸ“ˆ The Leap Forward: {.smaller}

1. ğŸ’¬ **Chatbots** : Basic Q&A capabilities, single-turn responses, limited context retention
   
2. ğŸ§  **Reasoning Engines** : Multi-step thinking, improved context handling, still confined in scope

3. ğŸš€ **AI Agents** : Proactive problem-solving, advanced tool utilization, autonomous goal pursuit 

ğŸ’¡ **Key Analogy:** Think of an AI Agent as a research assistant you can delegate tasks to, not just a calculator for computations.

## âš¡ Some example Research Agents {.smaller}

1. ğŸ“š **Automated Literature Synthesis** : Comprehensive source scanning, Automated citation management, Intelligent synthesis

2. ğŸ’» **Code Generation & Debugging** : Natural language to code, Multi-language support (Python, R, Stata), Intelligent debugging

3. ğŸ” **Autonomous Data Handling** : Automated data collection, Smart cleaning algorithms, Advanced analysis tools, Multi-source integration

4. ğŸ”„ **Workflow Orchestration** : End-to-end automation, Simulation management, Results analysis, Visualization generation

## ğŸ‘¨â€ğŸ’» What does using Agents look like? {.smaller}

1. ğŸ’» **Development Environment**: Modern IDEs integration, Seamless workflow integration, Real-time assistance

2. ğŸ¤– **Agent Interaction** : Chat-based interface, Customizable personas, Task-specific fine-tuning, Economic research specialization

3. ğŸŒ **Available Platforms** : ğŸ““ Notebook LM with research tools, ğŸ”§ Gemini/OpenAI/Anthropic integration, ğŸ› ï¸ VSCode with extensions (e.g., Clive), ğŸš€ Cursor's built-in agent support, AWS bedrock, Azure openai, Google vertex ai, Python frameworks like langchain.

4. ğŸ“Š **Deep research** : Gemini, Claude, OpenAI all have their own ways to do deep research. Its basically where you have an LLM attached to tool, you instruct it what to do then it returns with a report. I have used Gemini for deep research, the most. Use it for the planning phase of a tool build. 

## What are tools in the Agent contest? {.smaller}

- Tools are anything which the LLM has access to which it can use to complete a task 

- MCP is how LLMs access different tools, think of it as an api wich allows the LLM to interact with something  


## Context is King {.smaller}

- Context refers to the memory of an LLM. There are often limits to the context of different models  

- Hallucinations are more commond as we fill up the tokens the model is tracking. 

- LLM and agents excell at well scoped task where full context is given, creating a plan before executing is important to think through the task and the results.

- Documenataion and githhub can add context and add value to the agents 


## How can we trust AI based collaboration? {.smaller}

- ğŸ¤” Trust but verify. When creating a plan, always think about how to verify the results. 

- Do we have a dataset we trust? Then we have labeled data we can test against and create score against. 

- Create a test data to run a simulation to unit test the model being developed. 


## The need for Evaluation {.smaller}

- As we thinking about what we want agents to do, we need to think about how to evaluate them.


- Having general benchmarkets is important to help us understand the performance of the agents across different models and domains.

![](pics/roocode-evals-example.png)
::: {.footer}
Source: [RooCode](https://roocode.ai)
:::


## ğŸ”’ The Critical Importance of Data Security {.smaller}

LLMs from big providers collect your data and we need to be careful about what is shared. 

1. âš ï¸ **Security Challenges** : Public AI tool risks, data exposure concerns, privacy compliance, regulatory requirements

3. ğŸ’ª **AWS Bedrock Solution** : Private cloud environment, access to some but not all public models, secure infrastructure

4. âœ… **Key Guarantees** : No external data training, complete confidentiality, virtual private cloud, data integrity preservation


## ğŸ¥ Case Study: Analyzing a Hospital Merger {.smaller}

A research team evaluates economic impacts of a proposed hospital merger using AI-powered analysis.

**ğŸ”„ AI-Assisted Workflow**

1. ğŸ“ **Initial Task Delegation** : High level of payoff for human involvement. Set goals, plan and how to verify. Can use to highlight ambiguities in the plan. Which API should we use? Do we have test data? How should we validated? What metrics should we use? Whats the final product look lile? Do we have key figures we are interested? (Researcher as manager)

2. ğŸ” **Data Collection Phase** : ğŸ“Š Use a coding assistant (e.g. Cursor, VScode) to write a script to scrape news filings of hospitals mergers in the last year in and in this year. Get a test dataset to compare results against.

3. ğŸ“ˆ **Analysis & Visualization** : ğŸ“Š Given data collected, use coding assistant to write a report that give key stats and figures. Do for new year and last year, to give us a benchmark. Identify outlier or cases to validate by hand.

**Future steps**

Setup to run autonomously every X months. Researcher will review report and validated results, maybe through a random sampling of cases. 




## ğŸ”¬ Towards a virtual Lab{.smaller}

1. ğŸ›¡ï¸ **Core Definition** : A secure environment with access to specialized agents which agents can work within. Agents can coordinate and collaborate, learn autonomously and solve complex tasks.



::: {.r-stretch}
![](pics/changed-virtual-lab.png)
:::

::: {.footer}
Source: [Nature Paper](https://www.nature.com/articles/s41586-025-09442-9) | [GitHub Repository](https://github.com/zou-group/virtual-lab)
:::

## ğŸ¯ Key takeaways {.smaller}

1. ğŸš€ **Transformation**: Researchers are becoming managers adding value by providing expertise and guidance to the agents.

2. ğŸ”’ **Security & Trust** : Done in a sandboxed environment where AI agents can run analysis. Option to protect data from LLM provider. 

3. ğŸ’ª **Empowerment**: Knowing the what you want to do and how to verify lowers the barrier to using new toolset

## ğŸ¤” Discussion Points {.smaller}

1. ğŸ¯ **Quick Wins**
   - Which tasks to automate first? (merger checking, data downloading, closure researcher)
   - What's easily achievable?
   - How to measure success?

2. âš ï¸ **Challenges**
   - Handling hallucinations
   - Maintaining quality

3. ğŸš€ **Next Steps**
   - Choose tasks to focus an AI-enhacned workflow 
   - Which tools should the team adopt?
